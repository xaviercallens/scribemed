#!/usr/bin/env python3
"""
Test script for local Ollama + Whisper setup
"""
import sys
import os

# Add backend to path
sys.path.insert(0, os.path.join(os.path.dirname(__file__), 'backend'))

print("ğŸ§ª Testing Local LLM Setup (Ollama + Whisper)")
print("=" * 60)

# Test 1: Check Ollama availability
print("\n1ï¸âƒ£  Checking Ollama availability...")
try:
    import ollama
    models_response = ollama.list()
    # Get models from response object
    available_models = [m.model for m in models_response.models]
    print(f"   âœ… Ollama is running")
    print(f"   ğŸ“¦ Available models: {', '.join(available_models)}")
    
    # Check for recommended models
    if 'llama2:latest' in available_models:
        print("   âœ… llama2:latest found")
    elif 'mistral:7b-instruct' in available_models:
        print("   âœ… mistral:7b-instruct found")
    else:
        print("   âš ï¸  Recommended models not found")
        print("   ğŸ’¡ Run: ollama pull llama2")
        
except Exception as e:
    print(f"   âŒ Ollama not available: {e}")
    print("   ğŸ’¡ Start Ollama: ollama serve")
    sys.exit(1)

# Test 2: Test Ollama generation
print("\n2ï¸âƒ£  Testing Ollama text generation...")
try:
    response = ollama.chat(
        model='llama2:latest',
        messages=[
            {'role': 'user', 'content': 'Say "Hello from Llama!" in one sentence.'}
        ]
    )
    print(f"   âœ… Generation successful")
    print(f"   ğŸ“ Response: {response['message']['content'][:100]}...")
except Exception as e:
    print(f"   âŒ Generation failed: {e}")
    sys.exit(1)

# Test 3: Check Whisper
print("\n3ï¸âƒ£  Checking Whisper installation...")
try:
    import whisper
    print(f"   âœ… Whisper installed")
    print(f"   ğŸ“¦ Available models: tiny, base, small, medium, large")
except ImportError:
    print(f"   âŒ Whisper not installed")
    print(f"   ğŸ’¡ Run: pip install openai-whisper")
    sys.exit(1)

# Test 4: Test services
print("\n4ï¸âƒ£  Testing service initialization...")
try:
    os.environ['SECRET_KEY'] = 'test_key_12345678'
    os.environ['OLLAMA_MODEL'] = 'llama2:latest'
    
    from app.services.ollama_service import get_ollama_service
    from app.services.medical_notes import get_medical_note_service
    
    ollama_service = get_ollama_service()
    print(f"   âœ… Ollama service initialized")
    print(f"   ğŸ¤– Model: {ollama_service.model}")
    
    medical_service = get_medical_note_service()
    print(f"   âœ… Medical note service initialized")
    
except Exception as e:
    print(f"   âŒ Service initialization failed: {e}")
    import traceback
    traceback.print_exc()
    sys.exit(1)

# Test 5: Test medical note generation
print("\n5ï¸âƒ£  Testing medical note generation...")
try:
    sample_transcript = """
    Doctor: Hello, what brings you in today?
    Patient: I've had a sore throat for about a week and some mild fever.
    Doctor: Any allergies?
    Patient: Yes, I'm allergic to penicillin.
    Doctor: Okay, let me examine your throat. It looks red and inflamed.
    Patient: It hurts when I swallow.
    Doctor: I'll prescribe some ibuprofen for the pain and recommend rest.
    """
    
    print("   ğŸ”„ Generating SOAP note...")
    result = medical_service.generate_soap_note(sample_transcript)
    
    print(f"   âœ… SOAP note generated")
    print(f"   â±ï¸  Time: {result['generation_time_seconds']:.2f}s")
    print(f"   ğŸ¤– Model: {result['model_used']}")
    
    soap = result['soap_note']
    if soap.get('subjective'):
        print(f"   ğŸ“ Subjective: {soap['subjective'][:80]}...")
    if soap.get('assessment'):
        print(f"   ğŸ“ Assessment: {soap['assessment'][:80]}...")
        
except Exception as e:
    print(f"   âŒ Medical note generation failed: {e}")
    import traceback
    traceback.print_exc()

# Summary
print("\n" + "=" * 60)
print("ğŸ“Š Test Summary")
print("=" * 60)
print("âœ… Ollama: WORKING")
print("âœ… Whisper: INSTALLED")
print("âœ… Services: INITIALIZED")
print("âœ… Medical Notes: WORKING")
print("\nğŸ‰ Local LLM setup is ready!")
print("\nğŸ“ Configuration:")
print("   - Ollama URL: http://localhost:11434")
print("   - Model: llama2:latest")
print("   - Whisper: base model")
print("\nğŸš€ Start the server: ./start_server.sh")
